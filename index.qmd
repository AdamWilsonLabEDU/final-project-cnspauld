---
title: "Comparing Water Health in a Pre and Current COVID Landscape"
author: Caroline Spaulding
subtitle: A Study in in Buffalo, NY
date: today
date-format: long
---

# Introduction

\[\~ 200 words\]

Many factors can affect how fresh water quality and composition is affected. Some of these factors include, but are not limited to, pH and turbidity. The pH of water measures how acidic or basic it may be on a scale of 0 to 14, with 0 representing a solution that is highly acidic, and 14 representing a solution that is highly basic. 7 represents a solution that is neutral. In a healthy freshwater body, pH typically ranges from 6.5 to 8.5. Turbidity is essentially the measure of how clear a liquid is. The cloudier it is, typically the unhealthier the water may be. This usually means the water contains pollutants, or particulate matter such as silt and clay. In this project, I will analyze the differences from the years 2019 and 2020 to better understand how the COVID-19 lock down affected essential measures of water quality in the Buffalo area. I hypothesize that water quality will have improved during the height of the pandemic. This is because human activity was greatly reduced, with only essential personnel leaving their homes. With no human activity to contribute to the addition of pollutants into the water system, pH and turbidity should stabilize to healthy levels.

Clearly stated background and questions / hypotheses / problems being addressed. Sets up the analysis in an interesting and compelling way. Include figures if you like.

# Materials and methods

\[\~ 200 words\]

-Packages and data used:
The packages used in this project are ggplot2, sf, and leaflet.

-Packages explained:


```{r}
library(ggplot2)
library(sf)
library(leaflet)
waterkeeper_data_2019 <- read.csv("data/WaterKeeper_2019.csv")
# Convert the data into a spatial object (sf)
waterkeeper_data_2019_sf <- st_as_sf(waterkeeper_data_2019, coords = c("LONG", "LAT"), crs = 4326)
waterkeeper_data_2019_sf$LONG <- waterkeeper_data_2019$LONG 
waterkeeper_data_2019_sf$LAT <- waterkeeper_data_2019$LAT 
# Create an interactive leaflet map
leaflet(waterkeeper_data_2019_sf) %>%
  addTiles() %>%  # Add OpenStreetMap tiles as a base layer
  addCircleMarkers(
    ~LONG, ~LAT,     # Latitude and Longitude columns
    radius = 5,      # Size of the markers
    color = "blue",  # Marker color
    popup = ~Location      # Pop-up text (display the "id" column when you click a marker)
  ) %>%
  setView(lng = mean(waterkeeper_data_2019$LONG), lat = mean(waterkeeper_data_2019$LAT), zoom = 10)
```

The code above represents data gathered by the Buffalo Niagara Waterkeeper in 2019. The plot is labelled by location, so one can see what monitoring stations the water data was collected from.

```{r}
library(ggplot2)
library(sf)
library(leaflet)
specific_file <- read.csv("data/specific_file.csv")
#Convert the data into a spatial object
specific_file_2020_sf <- st_as_sf(specific_file, coords = c("longitude", "latitude"), crs = 4326)

specific_file_2020_sf$longitude <- specific_file$longitude 
specific_file_2020_sf$latitude <- specific_file$latitude

leaflet(specific_file_2020_sf) %>%
  addTiles() %>%
  addCircleMarkers(
    ~longitude, ~latitude,
    radius = 5,
    color = "red",
    popup = ~waterbody_name
  ) %>%
  setView(lng = mean(specific_file$longitude), lat = mean(specific_file$latitude), zoom = 10)
```

The code above shows data collected by the New York State Department of Environmental Conservation in 2020. This plot is also labelled by location, so one can see what body of water datat was collected from.

Narrative: Clear narrative description of the data sources and methods. Includes data from at least two sources that were integrated / merged in R.

Code: The code associated with the project is well organized and easy to follow. Demonstrates mastery of R graphics and functions.

Data: The underlying data are publicly accessible via the web and downloaded/accessed within the Rmd script. If you want to use your own data, you must make it available on a website (e.g. Figshare) so that others are able to re-run your code.

You can do bullets like this:

-   The first most important thing
-   The second most important thing
-   The third most important thing

You can do numbers like this:

1.  The first most important thing
2.  The second most important thing
3.  The third most important thing

See <http://rmarkdown.rstudio.com/> for all the amazing things you can do.

Here's my first code chunk.

```{r}
library(ggplot2)
library(sf)
waterkeeper_data_2019 <- read.csv("data/WaterKeeper_2019.csv")
# Convert the data into a spatial object (sf)
waterkeeper_data_2019_sf <- st_as_sf(waterkeeper_data_2019, coords = c("LONG", "LAT"), crs = 4326)
waterkeeper_data_2019_sf$LONG <- waterkeeper_data_2019$LONG 
waterkeeper_data_2019_sf$LAT <- waterkeeper_data_2019$LAT 
#Plotting turbidity over time for waterkeeper data
ggplot(waterkeeper_data_2019_sf, aes(x = waterkeeper_data_2019$Date, y = waterkeeper_data_2019$turbidity)) +
  geom_line(color = "blue") +  # Line plot
  geom_point(color = "red") +  # Scatter plot points
  labs(x = "Time", y = "Turbidity (NTU)", title = "Turbidity vs Time") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1))
```

Refer to output in your narrative like this: x=`r x` .

Load any required packages in a code chunk (you may need to install some packages):

```{r, message=F, warning=F}
library(tidyverse)
library(leaflet)
library(kableExtra)
library(htmlwidgets)
library(widgetframe)
knitr::opts_chunk$set(widgetframe_widgets_dir = 'widgets' ) 
knitr::opts_chunk$set(cache=TRUE)  # cache the results for quick compiling
```

## Download and clean all required data

```{r}
n=20
data=data.frame(x=runif(n,-180,180),
                y=runif(n,-60,60),
                size = runif(n, 5, 20),
                category = factor(
                  sample(letters[1:5], n, replace = TRUE)
                  ),
                value = rnorm(n))
```

```{r, results='asis'}
data %>% 
  slice(1:10) %>% #show only 1:n rows
  kable(digits=2,align="c")%>% #make table and round to two digits
  kable_styling(bootstrap_options = 
                  c("striped", "hover", "condensed", "responsive")) #apply other formatting
```

Add any additional processing steps here.

# Results

\[\~200 words\]

Tables and figures (maps and other graphics) are carefully planned to convey the results of your analysis. Intense exploration and evidence of many trials and failures. The author looked at the data in many different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.

```{r, fig.width=6, fig.height=3, fig.cap="Map of completely random data"}
m <- leaflet(data) %>% 
  addTiles() %>% 
  addCircleMarkers(~x, ~y, radius = ~size,color = ~as.factor(category)) %>% 
  addPopups(~x[2], ~y[2], "Random popup")
m  # a map with the default OSM tile layer
```

```{r}
data %>% 
  ggplot(aes(x=x,y=y,col=category))+
  geom_point()
```

### Dygraphs Example

```{r}
library(dygraphs)
dygraph(nhtemp, main = "New Haven Temperatures") |> 
  dyRangeSelector(dateWindow = c("1920-01-01", "1960-01-01")) 
```

# Conclusions

\[\~200 words\]

Clear summary adequately describing the results and putting them in context. Discussion of further questions and ways to continue investigation.

# References

All sources are cited in a consistent manner
